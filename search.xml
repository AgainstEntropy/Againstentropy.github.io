<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>CS231n-Assignment1 Softmax</title>
    <url>/2021/10/20/CS231n-Assignment1-Softmax/</url>
    <content><![CDATA[<h2 id="&#x6B63;&#x6587;"><a href="#&#x6B63;&#x6587;" class="headerlink" title="&#x6B63;&#x6587;"></a>&#x6B63;&#x6587;</h2><p>&#x5199;&#x4E00;&#x4E9B;&#x4E1C;&#x897F;&#xFF0C;&#x65B9;&#x4FBF;&#x7406;&#x89E3;&#x5982;&#x4F55;&#x6C42;Softmax&#x7684;cross_entropy_loss&#x548C;&#x6743;&#x91CD;&#x77E9;&#x9635;$W$&#x7684;&#x68AF;&#x5EA6;<code>dW</code>&#x3002;</p>
<h3 id="&#x635F;&#x5931;&#x51FD;&#x6570;"><a href="#&#x635F;&#x5931;&#x51FD;&#x6570;" class="headerlink" title="&#x635F;&#x5931;&#x51FD;&#x6570;"></a>&#x635F;&#x5931;&#x51FD;&#x6570;</h3><p>&#x9996;&#x5148;&#x9700;&#x8981;&#x660E;&#x786E;&#x7684;&#x662F;&#xFF0C;SVM&#x548C;Softmax&#x662F;&#x4E24;&#x79CD;&#x5206;&#x7C7B;&#x5668;&#xFF08;Classifier&#xFF09;&#x7684;&#x540D;&#x5B57;&#xFF0C;&#x5B83;&#x4EEC;&#x5206;&#x522B;&#x4F7F;&#x7528;&#x4E86;hinge loss&#xFF08;&#x6709;&#x65F6;&#x4E5F;&#x53EB;&#x505A;max-margin loss&#xFF09;&#x548C;cross entropy loss&#x4F5C;&#x4E3A;&#x5404;&#x81EA;&#x7684;&#x635F;&#x5931;&#x51FD;&#x6570;&#x3002;&#x4E24;&#x79CD;&#x5206;&#x7C7B;&#x5668;&#x901A;&#x8FC7;&#x4F18;&#x5316;&#x53C2;&#x6570;&#x6765;&#x51CF;&#x5C0F;&#x635F;&#x5931;&#x51FD;&#x6570;&#x7684;&#x503C;&#xFF0C;&#x6700;&#x7EC8;&#x90FD;&#x53EF;&#x4EE5;&#x8FBE;&#x5230;&#x5206;&#x7C7B;&#x7684;&#x76EE;&#x7684;&#x3002;</p>
<p>&#x4E0E;SVM&#x7684;hinge loss&#x4E0D;&#x540C;&#xFF0C;Softmax&#x91C7;&#x7528;cross entropy loss&#x6765;&#x8BC4;&#x4F30;&#x5206;&#x7C7B;&#x7ED3;&#x679C;&#x7684;&#x597D;&#x574F;&#x3002;&#x5BF9;&#x4E8E;&#x4E24;&#x4E2A;&#x7ED9;&#x5B9A;&#x7684;&#x5206;&#x5E03;$p(c)$&#x548C;$q(c)$&#xFF0C;&#x5B83;&#x4EEC;&#x4E4B;&#x95F4;&#x7684;<em>cross entropy</em>&#xFF08;&#x4EA4;&#x53C9;&#x71B5;&#xFF09;&#x4E3A;&#xFF1A;<br><!-- $$ --><br>\begin{equation}<br>H(p,q) = - \sum_c p(c) \log q(c)<br>\end{equation}<br><!-- $$ --><br><div class="note info no-icon"><p>&#x6CE8;&#xFF1A;&#x5982;&#x4E0D;&#x52A0;&#x8BF4;&#x660E;&#xFF0C;blog&#x4E2D;&#x7684;&#x6240;&#x6709;<em>log</em>&#x5747;&#x6307;&#x81EA;&#x7136;&#x5BF9;&#x6570;&#xFF0C;&#x5373; <em>$log \equiv log_e=ln$</em>&#x3002;</p>
</div></p>
<p>&#x4F8B;&#x5982;&#xFF0C;&#x5F53;&#x6211;&#x4EEC;&#x60F3;&#x8981;&#x8BC4;&#x4F30;&#x7B2C;$i$&#x5F20;&#x56FE;&#x7247;&#x662F;&#x5426;&#x5DF2;&#x7ECF;&#x88AB;&#x5408;&#x7406;&#x5206;&#x7C7B;&#x65F6;&#xFF0C;&#x5B83;&#x7684;&#x771F;&#x5B9E;&#x5206;&#x5E03;$p_i$&#x5E94;&#x5F53;&#x662F;&#x4E00;&#x4E2A;&#x5F62;&#x5982;$[0, \ldots 1, \ldots, 0]$&#x7684;&#x5411;&#x91CF;&#xFF0C;&#x5176;&#x4E2D;&#x53EA;&#x6709;$y_i$&#x5904;&#x4E3A;1&#xFF0C;&#x5176;&#x4ED6;&#x4F4D;&#x7F6E;&#x90FD;&#x4E3A;0&#xFF0C;&#x8FD9;&#x610F;&#x5473;&#x7740;&#x7B2C;$i$&#x5F20;&#x56FE;&#x7247;&#x7684;&#x771F;&#x5B9E;&#x60C5;&#x51B5;&#x662F;&#x201C;&#x6709;100%&#x7684;&#x6982;&#x7387;&#x5904;&#x4E8E;&#x7B2C;$y_i$&#x7C7B;&#x201D;&#xFF0C;&#x4E5F;&#x5373;&#x4E00;&#x4E2A;$\delta$&#x5206;&#x5E03;&#x3002;&#x8981;&#x8BA1;&#x7B97;&#x4EA4;&#x53C9;&#x71B5;&#xFF0C;&#x6211;&#x4EEC;&#x9996;&#x5148;&#x8FD8;&#x9700;&#x8981;&#x6C42;&#x51FA;&#x5F53;&#x524D;&#x7684;&#x5206;&#x5E03;$q_i$&#x3002;&#x6839;&#x636E;<a href>SVM</a>&#x7BC7;&#x4E2D;&#x5F97;&#x5230;&#x7684;&#x5F97;&#x5206;&#x77E9;&#x9635;$S$&#xFF0C;&#x6211;&#x4EEC;&#x53EF;&#x4EE5;&#x5229;&#x7528;&#x7B2C;$i$&#x5F20;&#x56FE;&#x7247;&#x5728;&#x5404;&#x4E2A;&#x5206;&#x7C7B;&#x4E0A;&#x7684;&#x5F97;&#x5206;$S_i$&#x5B9A;&#x91CF;&#x201C;&#x4F30;&#x8BA1;&#x201D;&#x51FA;$q_i$&#xFF0C;&#x5373;&#xFF1A;<br><!-- $$ --><br>\begin{equation}<br>q_i(c) = \frac{e^{S_{i c}}}{\sum_c e^{S_{i c}}}<br>\end{equation}<br><!-- $$ --><br>&#x8FD9;&#x91CC;$c$&#x7528;&#x6765;&#x6807;&#x8BB0;&#x5404;&#x4E2A;&#x5206;&#x7C7B;&#x3002;&#x8FD9;&#x79CD;&#x6982;&#x7387;&#x8BE0;&#x91CA;&#x975E;&#x5E38;&#x7C7B;&#x4F3C;&#x4E8E;&#x7EDF;&#x8BA1;&#x7269;&#x7406;&#x4E2D;<a href="https://en.wikipedia.org/wiki/Partition_function">&#x914D;&#x5206;&#x51FD;&#x6570;</a>&#x7684;&#x5F62;&#x5F0F; &#x2014;&#x2014; $e$&#x6307;&#x6570;&#x4FDD;&#x8BC1;&#x4E86;&#x6982;&#x7387;&#x4E3A;&#x6B63;&#x503C;&#xFF0C;&#x5206;&#x6BCD;&#x4FDD;&#x8BC1;&#x4E86;&#x6982;&#x7387;&#x5F52;&#x4E00;&#x5316;&#xFF0C;&#x5373;$\sum_c q_i(c) = 1$&#x3002;</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="comment"># &#x56FA;&#x5B9A;numpy&#x968F;&#x673A;&#x6570;&#x79CD;&#x5B50;</span></span><br><span class="line">np.random.seed(<span class="number">425</span>)</span><br><span class="line"><span class="comment"># &#x968F;&#x673A;&#x751F;&#x6210;&#x4E00;&#x4E9B;&#x56FE;&#x7247;&#x548C;&#x6807;&#x7B7E;</span></span><br><span class="line">num = <span class="number">10</span></span><br><span class="line">imgs = np.random.rand(num, <span class="number">2</span>, <span class="number">2</span>, <span class="number">3</span>)          <span class="comment"># 10&#x5F20;dim=(2,2,3)&#x7684;&#x56FE;&#x7247;</span></span><br><span class="line">labels = np.random.randint(<span class="number">0</span>, <span class="number">3</span>, size=num)   <span class="comment"># &#x5171;&#x5206;&#x4E3A;3&#x7C7B;</span></span><br><span class="line"><span class="comment"># &#x6570;&#x636E;&#x9884;&#x5904;&#x7406;</span></span><br><span class="line">imgs = imgs.reshape(num, -<span class="number">1</span>)</span><br><span class="line">imgs = np.hstack([imgs, np.ones((num, <span class="number">1</span>))])</span><br><span class="line"><span class="comment"># &#x968F;&#x673A;&#x521D;&#x59CB;&#x5316;&#x4E00;&#x4E2A;&#x6743;&#x91CD;&#x77E9;&#x9635;W</span></span><br><span class="line">W = <span class="number">0.1</span> * np.random.randn(<span class="number">13</span>, <span class="number">3</span>)</span><br><span class="line"><span class="comment"># &#x8BA1;&#x7B97;&#x5F97;&#x5206;&#x77E9;&#x9635;</span></span><br><span class="line">S = np.dot(imgs, W)</span><br><span class="line"><span class="comment"># &#x8BA1;&#x7B97;&#x6982;&#x7387;&#x5206;&#x5E03;&#x77E9;&#x9635;&#x201C;q_ic&#x201D;</span></span><br><span class="line">Q = np.exp(S)</span><br><span class="line">Q /= np.<span class="built_in">sum</span>(Q, axis=<span class="number">1</span>, keepdims=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;\nQ&#x662F;&#x4E00;&#x4E2A;<span class="subst">{Q.shape}</span>&#x7EF4;&#x77E9;&#x9635;\n<span class="subst">{Q}</span>&quot;</span>)</span><br></pre></td></tr></table></figure>
<pre><code>Q&#x662F;&#x4E00;&#x4E2A;(10,3)&#x7EF4;&#x77E9;&#x9635;
[[0.45059057 0.2733205  0.27608894]
 [0.44649569 0.28797703 0.26552728]
 [0.37655887 0.31709582 0.30634531]
 [0.43796601 0.30100174 0.26103225]
 [0.4468077  0.25942023 0.29377208]
 [0.43900853 0.28268161 0.27830985]
 [0.43906041 0.25220238 0.30873721]
 [0.42094716 0.30467527 0.27437757]
 [0.35515894 0.34033532 0.30450574]
 [0.40898969 0.29244079 0.29856952]]
</code></pre><p>&#x53EF;&#x4EE5;&#x770B;&#x5230;&#x6982;&#x7387;&#x5206;&#x5E03;&#x77E9;&#x9635;<code>Q</code>&#x7684;&#x6BCF;&#x4E00;&#x884C;&#x90FD;&#x662F;&#x5F52;&#x4E00;&#x5316;&#x7684;&#xFF0C;&#x5E76;&#x4E14;&#x7531;&#x4E8E;&#x4F7F;&#x7528;&#x8F83;&#x5C0F;&#x7684;&#x6570;&#x5B57;&#x8FDB;&#x884C;&#x6743;&#x91CD;&#x77E9;&#x9635;<code>W</code>&#x7684;&#x968F;&#x673A;&#x521D;&#x59CB;&#x5316;&#xFF0C;&#x6BCF;&#x5F20;&#x56FE;&#x7247;&#x5728;&#x6BCF;&#x4E2A;&#x5206;&#x7C7B;&#x4E0A;&#x7684;&#x6982;&#x7387;&#x90FD;&#x5728;$33\%$&#x5DE6;&#x53F3;&#x3002;&#x73B0;&#x5728;&#x53EF;&#x4EE5;&#x5199;&#x51FA;&#x7B2C;$i$&#x5F20;&#x56FE;&#x7247;&#x7684;&#x4EA4;&#x53C9;&#x71B5;&#x635F;&#x5931;&#xFF1A;<br><!-- $$ --><br>\begin{equation}<br>L_i = - \log q_i(y_i) = - \log \left(\frac{e^{S_{i y_i}}}{\sum_c e^{S_{i c}}} \right) = -S_{i y_i} + \log\sum_c e^{S_{i c}}<br>\end{equation}<br><!-- $$ --></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">L = -np.log(Q[<span class="built_in">range</span>(Q.shape[<span class="number">0</span>]), labels])</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;&#x6BCF;&#x5F20;&#x56FE;&#x7247;&#x7684;&#x635F;&#x5931;&#x4E3A;&#xFF1A;\nL = <span class="subst">{L}</span>\n&quot;</span>)</span><br><span class="line">loss = np.<span class="built_in">sum</span>(L)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;&#x603B;&#x635F;&#x5931;&#x4E3A;&#xFF1A;\nloss = <span class="subst">{loss}</span>&quot;</span>)</span><br></pre></td></tr></table></figure>
<pre><code>&#x6BCF;&#x5F20;&#x56FE;&#x7247;&#x7684;&#x635F;&#x5931;&#x4E3A;&#xFF1A;
L = [1.29711019 1.32603769 1.14855128 1.20063924 0.80562698 1.2790202
     1.3775234  1.18850876 1.03518988 1.22949307]

&#x603B;&#x635F;&#x5931;&#x4E3A;&#xFF1A;
loss = 11.887700678860678
</code></pre><h3 id="&#x68AF;&#x5EA6;"><a href="#&#x68AF;&#x5EA6;" class="headerlink" title="&#x68AF;&#x5EA6;"></a>&#x68AF;&#x5EA6;</h3><p>&#x73B0;&#x5728;&#x6765;&#x8BA1;&#x7B97;&#x68AF;&#x5EA6;&#x77E9;&#x9635;<code>dW</code>&#x3002;&#x53C2;&#x8003;SVM&#x7BC7;&#x4E2D;&#x7ED9;&#x51FA;&#x7684;&#x68AF;&#x5EA6;&#x7684;&#x5B9A;&#x4E49;&#xFF0C;<code>dW</code>&#x7684;&#xFF08;&#x77E2;&#x91CF;&#x5316;&#xFF09;&#x8BA1;&#x7B97;&#x516C;&#x5F0F;&#x4E3A;&#xFF1A;<br><!-- $$ --><br>\begin{equation}<br>    \nabla_{W} L<br>    :=\frac{dL}{dW}=<br>    \frac{dS}{dW} \frac{\partial L}{\partial S} + \frac{\partial L}{\partial W}<br>\end{equation}<br><!-- $$ --><br>&#x5176;&#x4E2D;<br><!-- $$ --><br>\begin{equation}<br>    \frac{dS}{dW} \equiv X^T \quad,\quad \frac{\partial L}{\partial W} = 2\lambda W<br>\end{equation}<br><!-- $$ --><br>&#x4E3A;&#x4E86;&#x6C42;&#x51FA;$\frac{\partial L}{\partial S}$&#xFF0C;&#x6211;&#x4EEC;&#x9996;&#x5148;&#x5206;&#x6790;$L_i$&#x7684;&#x8D21;&#x732E;&#x3002;&#x7531;&#x4E8E;$L_i$&#x4EC5;&#x4E0E;&#x5F97;&#x5206;&#x77E9;&#x9635;$S$&#x7684;&#x7B2C;$i$&#x884C;&#x6709;&#x5173;&#x7CFB;&#xFF0C;&#x56E0;&#x6B64;<br><!-- $$ --><br>\begin{equation}<br>    \frac{\partial L_i}{\partial S_{id}} = -\delta_{dy_i} + \frac{e^{S_{id}}}{\sum_c e^{S_{i c}}} = -\delta_{dy_i} + q_i(d)<br>\end{equation}<br><!-- $$ --><br>&#x5BF9;$i$&#x6C42;&#x548C;&#xFF0C;&#x5373;&#x53EF;&#x53D1;&#x73B0;&#x5B8C;&#x6574;&#x7684;$\frac{\partial L}{\partial S}$&#x5C31;&#x662F;&#x5728;&#x6982;&#x7387;&#x5206;&#x5E03;&#x77E9;&#x9635;<code>Q</code>&#x7684;&#x57FA;&#x7840;&#x4E0A;&#xFF0C;&#x5C06;&#x6B63;&#x786E;&#x5206;&#x7C7B;&#x4F4D;&#x7F6E;&#x51CF;&#x4E00;&#x5373;&#x53EF;&#x3002;&#x8BA1;&#x7B97;&#x68AF;&#x5EA6;&#x77E9;&#x9635;<code>dW</code>&#x7684;&#x4EE3;&#x7801;&#x4E3A;&#xFF1A;</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">Q[np.arange(num), y] -= <span class="number">1</span></span><br><span class="line">dW = np.dot(X.T, Q)</span><br><span class="line"></span><br><span class="line">dW /= num           <span class="comment"># &#x8BA1;&#x7B97;&#x635F;&#x5931;&#x9700;&#x8981;&#x9664;&#x4EE5;&#x56FE;&#x7247;&#x6570;&#x91CF;</span></span><br><span class="line">dW += <span class="number">2</span> * reg * W   <span class="comment"># &#x8FD8;&#x9700;&#x8981;&#x52A0;&#x4E0A;&#x6B63;&#x5219;&#x5316;&#x635F;&#x5931;</span></span><br></pre></td></tr></table></figure>
<h3 id="&#x8BAD;&#x7EC3;"><a href="#&#x8BAD;&#x7EC3;" class="headerlink" title="&#x8BAD;&#x7EC3;"></a>&#x8BAD;&#x7EC3;</h3><p>&#x6700;&#x540E;&#x5199;&#x51FA;&#x5B8C;&#x6574;&#x7684;&#x4EE3;&#x7801;&#xFF1A;</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">softmax_loss_grad</span>(<span class="params">W, X, y, reg</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    Structured Softmax loss function, vectorized implementation.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Inputs have dimension D, there are C classes, and we operate on minibatches </span></span><br><span class="line"><span class="string">    of N examples.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Inputs:</span></span><br><span class="line"><span class="string">    - W: A numpy array of shape (D, C) containing weights.</span></span><br><span class="line"><span class="string">    - X: A numpy array of shape (N, D) containing a minibatch of data.</span></span><br><span class="line"><span class="string">    - y: A numpy array of shape (N,) containing training labels; y[i] = c means</span></span><br><span class="line"><span class="string">      that X[i] has label c, where 0 &lt;= c &lt; C.</span></span><br><span class="line"><span class="string">    - reg: (float) regularization strength</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Returns a tuple of:</span></span><br><span class="line"><span class="string">    - loss as single float</span></span><br><span class="line"><span class="string">    - gradient with respect to weights W; an array of same shape as W</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="comment"># Initialize the loss and gradient to zero.</span></span><br><span class="line">    loss = <span class="number">0.0</span></span><br><span class="line">    dW = np.zeros_like(W)</span><br><span class="line"></span><br><span class="line">    N, D = X.shape</span><br><span class="line">    <span class="comment"># &#x8BA1;&#x7B97;&#x5F97;&#x5206;&#x77E9;&#x9635;</span></span><br><span class="line">    S = np.dot(imgs, W)</span><br><span class="line">    <span class="comment"># &#x8BA1;&#x7B97;&#x6982;&#x7387;&#x5206;&#x5E03;&#x77E9;&#x9635;&#x201C;q_ic&#x201D;</span></span><br><span class="line">    Q = np.exp(S)</span><br><span class="line">    Q /= np.<span class="built_in">sum</span>(Q, axis=<span class="number">1</span>, keepdims=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">    L = -np.log(Q[<span class="built_in">range</span>(Q.shape[<span class="number">0</span>]), labels])</span><br><span class="line">    loss += np.<span class="built_in">sum</span>(L)</span><br><span class="line">    loss /= N</span><br><span class="line">    loss += <span class="number">0.5</span> * reg * np.<span class="built_in">sum</span>(W ** <span class="number">2</span>)</span><br><span class="line"></span><br><span class="line">    Q[np.arange(num), y] -= <span class="number">1</span></span><br><span class="line">    dW = np.dot(X.T, Q) / N <span class="comment"># &#x8BA1;&#x7B97;&#x635F;&#x5931;&#x9700;&#x8981;&#x9664;&#x4EE5;&#x56FE;&#x7247;&#x6570;&#x91CF;</span></span><br><span class="line">    dW += <span class="number">2</span> * reg * W       <span class="comment"># &#x8FD8;&#x9700;&#x8981;&#x52A0;&#x4E0A;&#x6B63;&#x5219;&#x5316;&#x635F;&#x5931;</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> loss, dW</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># &#x56FA;&#x5B9A;numpy&#x968F;&#x673A;&#x6570;&#x79CD;&#x5B50;</span></span><br><span class="line">np.random.seed(<span class="number">425</span>)</span><br><span class="line"><span class="comment"># &#x968F;&#x673A;&#x751F;&#x6210;&#x4E00;&#x4E9B;&#x56FE;&#x7247;&#x548C;&#x6807;&#x7B7E;</span></span><br><span class="line">num = <span class="number">10</span></span><br><span class="line">imgs = np.random.rand(num, <span class="number">2</span>, <span class="number">2</span>, <span class="number">3</span>)          <span class="comment"># 10&#x5F20;dim=(2,2,3)&#x7684;&#x56FE;&#x7247;</span></span><br><span class="line">labels = np.random.randint(<span class="number">0</span>, <span class="number">3</span>, size=num)   <span class="comment"># &#x5171;&#x5206;&#x4E3A;3&#x7C7B;</span></span><br><span class="line"><span class="comment"># &#x6570;&#x636E;&#x9884;&#x5904;&#x7406;</span></span><br><span class="line">imgs = imgs.reshape(num, -<span class="number">1</span>)</span><br><span class="line">imgs = np.hstack([imgs, np.ones((num, <span class="number">1</span>))])</span><br><span class="line"><span class="comment"># &#x968F;&#x673A;&#x521D;&#x59CB;&#x5316;&#x4E00;&#x4E2A;&#x6743;&#x91CD;&#x77E9;&#x9635;W</span></span><br><span class="line">W = <span class="number">0.1</span> * np.random.randn(<span class="number">13</span>, <span class="number">3</span>)</span><br><span class="line"></span><br><span class="line">loss_list, correct_rate_list = [], []</span><br><span class="line">T = np.arange(<span class="number">200</span>)  <span class="comment"># &#x8BAD;&#x7EC3;&#x6B21;&#x6570;</span></span><br><span class="line">alpha = <span class="number">0.1</span>         <span class="comment"># &#x5B66;&#x4E60;&#x7387;&#xFF08;&#x6B65;&#x957F;&#xFF09;</span></span><br><span class="line">reg = <span class="number">5e-5</span>          <span class="comment"># &#x6B63;&#x5219;&#x60E9;&#x7F5A;&#x5F3A;&#x5EA6;</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> t <span class="keyword">in</span> T:</span><br><span class="line">    loss, dW = softmax_loss_grad(W, imgs, labels, reg=reg)</span><br><span class="line">    W -= alpha * dW</span><br><span class="line"></span><br><span class="line">    scores = np.dot(imgs, W)</span><br><span class="line">    labels_pred = np.argmax(scores, axis=<span class="number">1</span>)</span><br><span class="line">    correct_rate = np.mean(labels_pred == labels)</span><br><span class="line"></span><br><span class="line">    loss_list.append(loss)</span><br><span class="line">    correct_rate_list.append(correct_rate)</span><br><span class="line"></span><br><span class="line">fig = plt.figure()</span><br><span class="line">plt.subplot(<span class="number">1</span>,<span class="number">2</span>,<span class="number">1</span>)</span><br><span class="line">plt.plot(T, loss_list)</span><br><span class="line">plt.title(<span class="string">&quot;loss&quot;</span>)</span><br><span class="line"></span><br><span class="line">plt.subplot(<span class="number">1</span>,<span class="number">2</span>,<span class="number">2</span>)</span><br><span class="line">plt.plot(T, correct_rate_list)</span><br><span class="line">plt.title(<span class="string">&quot;correct_rate&quot;</span>)</span><br><span class="line"></span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<p><img src="/2021/10/20/CS231n-Assignment1-Softmax/loss_and_train.png" alt="png"></p>
<h3 id="&#x5176;&#x4ED6;&#x7EC6;&#x8282;"><a href="#&#x5176;&#x4ED6;&#x7EC6;&#x8282;" class="headerlink" title="&#x5176;&#x4ED6;&#x7EC6;&#x8282;"></a>&#x5176;&#x4ED6;&#x7EC6;&#x8282;</h3><h4 id="&#x6570;&#x503C;&#x7A33;&#x5B9A;&#x6027;&#x95EE;&#x9898;"><a href="#&#x6570;&#x503C;&#x7A33;&#x5B9A;&#x6027;&#x95EE;&#x9898;" class="headerlink" title="&#x6570;&#x503C;&#x7A33;&#x5B9A;&#x6027;&#x95EE;&#x9898;"></a>&#x6570;&#x503C;&#x7A33;&#x5B9A;&#x6027;&#x95EE;&#x9898;</h4><p>&#x5728;&#x8BA1;&#x7B97;Softmax&#x7684;<em>Cross Entropy loss</em>&#x65F6;&#xFF0C;&#x7531;&#x4E8E;&#x91C7;&#x7528;&#x4E86;&#x6307;&#x6570;&#x8FD0;&#x7B97;&#xFF0C;&#x5F53;&#x5F97;&#x5206;&#x77E9;&#x9635;&#x7684;&#x5143;&#x7D20;&#x8F83;&#x5927;&#x65F6;&#xFF0C;&#x5BB9;&#x6613;&#x5BFC;&#x81F4;&#x9664;&#x6570;&#x8FC7;&#x5927;&#x65F6;&#x7684;&#x4E0D;&#x7A33;&#x5B9A;&#xFF0C;&#x8FD0;&#x7B97;&#x7CBE;&#x5EA6;&#x53EF;&#x80FD;&#x4F1A;&#x53D7;&#x5230;&#x5F71;&#x54CD;&#xFF0C;&#x56E0;&#x6B64;&#x9700;&#x8981;&#x7528;&#x5230;&#x4E00;&#x4E9B;&#x5B9E;&#x7528;&#x6280;&#x5DE7;&#x3002;&#x6CE8;&#x610F;&#x5230;<br><!-- $$ --><br>\begin{equation}<br>q_i(c) = \frac{e^{S_{i c}}}{\sum_c e^{S_{i c}}} = \frac{e^A e^{S_{i c}}}{e^A \sum_c e^{S_{i c}}} = \frac{e^{S_{i c}+A}}{\sum_c e^{S_{i c}+A}}<br>\end{equation}<br><!-- $$ --><br>&#x6211;&#x4EEC;&#x53EF;&#x4EE5;&#x9009;&#x62E9;&#x4EFB;&#x610F;&#x7684;$A$&#x800C;&#x4E0D;&#x4F1A;&#x5BF9;&#x7ED3;&#x679C;&#x9020;&#x6210;&#x4EFB;&#x4F55;&#x5F71;&#x54CD;&#x3002;&#x56E0;&#x6B64;&#x4E3A;&#x4E86;&#x9632;&#x6B62;&#x6307;&#x6570;&#x8FD0;&#x7B97;&#x540E;&#x6570;&#x503C;&#x8FC7;&#x5927;&#xFF0C;&#x5728;&#x8BA1;&#x7B97;$q_i$&#x65F6;&#x53EF;&#x4EE5;&#x9009;&#x62E9;$A=-\max S_i$&#xFF0C;&#x8FD9;&#x6837;&#x5C31;&#x4FDD;&#x8BC1;&#x4E86;$e^{S_{i c}+A} \leq 1, \forall c$&#x3002;&#x53EA;&#x9700;&#x5728;&#x5B8C;&#x6574;&#x4EE3;&#x7801;&#x7684;&#x7B2C;28&#x884C;&#x540E;&#x63D2;&#x5165;&#x4EE5;&#x4E0B;&#x4EE3;&#x7801;&#x5373;&#x53EF;&#x3002;</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">S -= np.<span class="built_in">max</span>(S, axis=<span class="number">1</span>, keepdims=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure>
<h4 id="SVM-&#x4E0E;-Softmax-&#x7684;&#x4E0D;&#x540C;"><a href="#SVM-&#x4E0E;-Softmax-&#x7684;&#x4E0D;&#x540C;" class="headerlink" title="SVM &#x4E0E; Softmax &#x7684;&#x4E0D;&#x540C;"></a>SVM &#x4E0E; Softmax &#x7684;&#x4E0D;&#x540C;</h4><p>&#x5728;SVM&#x4E2D;&#xFF0C;&#x53EA;&#x8981;&#x6B63;&#x786E;&#x5206;&#x7C7B;&#x7684;&#x5F97;&#x5206;$S_{iy_i}$&#x6BD4;&#x5176;&#x4ED6;&#x5206;&#x7C7B;&#x5F97;&#x5206;$S_{ic}(c\neq y_i)$&#x9AD8;&#x51FA;&#x4E00;&#x5B9A;&#x9650;&#x503C;&#xFF08;&#x5373;&#x8BBE;&#x5B9A;&#x7684;$\Delta$&#xFF09;&#xFF0C;&#x5C31;&#x8BA4;&#x4E3A;&#x7B2C;$i$&#x5F20;&#x56FE;&#x7247;&#x5DF2;&#x7ECF;&#x88AB;&#x6B63;&#x786E;&#x5206;&#x7C7B;&#xFF0C;&#x6B64;&#x65F6;&#x7B2C;$i$&#x5F20;&#x56FE;&#x7247;&#x5C06;&#x4E0D;&#x4F1A;&#x5BF9;&#x603B;&#x635F;&#x5931;&#x51FD;&#x6570;&#x505A;&#x51FA;&#x4EFB;&#x4F55;&#x8D21;&#x732E;&#xFF1B;&#x4F46;Softmax&#x5219;&#x4E0D;&#x540C;&#xFF0C;&#x6307;&#x6570;&#x51FD;&#x6570;&#x7684;&#x6B63;&#x503C;&#x6027;&#x4FDD;&#x8BC1;&#x4E86;&#x5F53;&#x524D;&#x6982;&#x7387;&#x5206;&#x5E03;$q$&#x4E0D;&#x53EF;&#x80FD;&#x4E0E;&#x771F;&#x5B9E;&#x7684;$\delta$&#x5206;&#x5E03;$p$&#x5B8C;&#x5168;&#x76F8;&#x540C;&#xFF0C;&#x56E0;&#x6B64;&#x603B;&#x4F1A;&#x5B58;&#x5728;&#x4EA4;&#x53C9;&#x71B5;&#x635F;&#x5931;&#x3002;</p>
<p>&#x8FD9;&#x79CD;&#x533A;&#x522B;&#x610F;&#x5473;&#x7740;&#x5728;&#x5BF9;&#x635F;&#x5931;&#x51FD;&#x6570;&#x505A;&#x4F18;&#x5316;&#x65F6;&#xFF0C;SVM&#x53EF;&#x80FD;&#x51FA;&#x73B0;<em>hinge loss</em>&#x5DF2;&#x7ECF;&#x88AB;&#x4F18;&#x5316;&#x4E3A;0&#x7684;&#x60C5;&#x51B5;&#xFF0C;&#x603B;&#x635F;&#x5931;&#x5168;&#x90E8;&#x4E3A;&#x6B63;&#x5219;&#x5316;&#x60E9;&#x7F5A;&#xFF1B;&#x800C;Softmax&#x5219;&#x4E0D;&#x4F1A;&#xFF0C;<em>cross entropy loss</em>&#x4FDD;&#x8BC1;&#x4E86;&#x6A21;&#x578B;&#x59CB;&#x7EC8;&#x53EF;&#x4EE5;&#x7EE7;&#x7EED;&#x4F18;&#x5316;&#xFF0C;&#x5E76;&#x4E14;&#x968F;&#x7740;&#x4F18;&#x5316;&#x7684;&#x8FDB;&#x884C;&#xFF0C;&#x635F;&#x5931;&#x51FD;&#x6570;&#x7684;&#x4E0B;&#x964D;&#x4F1A;&#x81EA;&#x53D1;&#x5448;&#x73B0;&#x4E0B;&#x964D;&#x51CF;&#x7F13;&#x7684;&#x8D8B;&#x52BF;&#x3002;</p>
<p><img src="/2021/10/20/CS231n-Assignment1-Softmax/svmvssoftmax.png" alt="png"></p>
<h2 id="&#x53C2;&#x8003;"><a href="#&#x53C2;&#x8003;" class="headerlink" title="&#x53C2;&#x8003;"></a>&#x53C2;&#x8003;</h2><ul>
<li>CS231n - Linear Classification<br><a href="https://cs231n.github.io/linear-classify/">https://cs231n.github.io/linear-classify/</a></li>
</ul>
]]></content>
      <tags>
        <tag>ML</tag>
        <tag>CV</tag>
      </tags>
  </entry>
  <entry>
    <title>CS231n-Assignment1 SVM</title>
    <url>/2021/10/14/SVM/</url>
    <content><![CDATA[<!-- 
<div style="width:100%;overflow-x:auto;overflow-y:hidden;">
<div id="calendar" style="width: 600px;height:185px;"></div>
</div>
<script src="https://cdn.jsdelivr.net/npm/echarts@4.8.0/dist/echarts.min.js"></script>
<script type="text/javascript">
  (function(){
    function dateFormat(date){
      date = new Date(date)
      return date.getFullYear() + '-' + (date.getMonth() + 1).toString().padStart(2, '0') + '-' + date.getDate().toString().padStart(2, '0')
    }
    let calendarChart = echarts.init(document.getElementById('calendar'));
    let endDate = new Date().getTime()
    let startDate = new Date(endDate - 40*7*24*3600*1000).getTime()
    let startDay = Math.ceil(startDate/(24*3600*1000))
    let endDay = Math.ceil(endDate/(24*3600*1000))

    let commitData = {"2021-11-04":4,"2021-10-17":2,"2021-10-15":5,"2021-10-14":4,"2021-10-13":5}
    let seriesData = []

    for(let i = startDay;i <= endDay;i++){
      let date = i*24*3600*1000
      let formatDate = dateFormat(date)
      let times = commitData[formatDate] || 0
      seriesData.push([formatDate, times])
    }
    let option = {
      title: {
        text: "活动日历",
        x: "center"
      },
      backgroundColor: "#f9f9f9",
      tooltip: {
        padding: 10,
        backgroundColor: "#555",
        borderColor: "#777",
        borderWidth: 1,
        formatter: function(a) {
          var b = a.value;
          return '<div style="font-size: 14px;">' + b[0] + "：" + b[1] + "</div>"
        }
      },
      visualMap: {
        show: !1,
        showLabel: !0,
        min: 0,
        max: 4,
        calculable: !1,
        inRange: {
          symbol: "rect",
          color: ["#ebedf0", "#c6e48b", "#7bc96f", "#239a3b", "#196127"]
        },
        itemWidth: 12,
        itemHeight: 12,
        orient: "horizontal",
        left: "center",
        top: 0
      },
      calendar: [{
        top: 50,
        left: "center",
        range: [dateFormat(startDate), dateFormat(endDate)],
        cellSize: [13, 13],
        splitLine: {
          show: !1
        },
        name: {
          textStyle: {
            color: "#3C4858"
          }
        },
        itemStyle: {
          borderColor: "#fff",
          borderWidth: 2
        },
        yearLabel: {
          show: !1
        },
        monthLabel: {
          nameMap: "cn",
          fontSize: 11,
          color: "#3C4858"
        },
        dayLabel: {
          formatter: "{start} 1st",
          nameMap: "cn",
          fontSize: 11,
          color: "#3C4858"
        }
      }],
      series: [{
        type: "heatmap",
        coordinateSystem: "calendar",
        calendarIndex: 0,
        data: seriesData
      }]
    };

    calendarChart.setOption(option);
  })();
</script>
 -->
<h2 id="&#x6B63;&#x6587;"><a href="#&#x6B63;&#x6587;" class="headerlink" title="&#x6B63;&#x6587;"></a>&#x6B63;&#x6587;</h2><p>&#x5199;&#x4E00;&#x4E9B;&#x4E1C;&#x897F;&#xFF0C;&#x65B9;&#x4FBF;&#x7406;&#x89E3;&#x5982;&#x4F55;&#x6C42;&#x7EBF;&#x6027;&#x5206;&#x7C7B;&#x5668;SVM&#x7684;hinge_loss&#x548C;&#x6743;&#x91CD;&#x77E9;&#x9635;$W$&#x7684;&#x68AF;&#x5EA6;<code>dW</code>&#x3002;</p>
<h3 id="&#x6570;&#x636E;"><a href="#&#x6570;&#x636E;" class="headerlink" title="&#x6570;&#x636E;"></a>&#x6570;&#x636E;</h3><h4 id="&#x751F;&#x6210;&#x6570;&#x636E;"><a href="#&#x751F;&#x6210;&#x6570;&#x636E;" class="headerlink" title="&#x751F;&#x6210;&#x6570;&#x636E;"></a>&#x751F;&#x6210;&#x6570;&#x636E;</h4><p>&#x5047;&#x8BBE;&#x6211;&#x4EEC;&#x5171;&#x6709;10&#x5F20;&#x56FE;&#x7247;&#xFF0C;&#x88AB;&#x5206;&#x4E3A;3&#x7C7B;&#x3002;&#x7528;$X_i$(<code>imgs[i](i=1,2,...,10)</code>)&#x8868;&#x793A;&#x7B2C;$i$&#x5F20;&#x56FE;&#x7247;&#x7684;&#x6240;&#x6709;&#x50CF;&#x7D20;&#xFF0C;$y_i$(<code>labels[i]</code>)&#x8868;&#x793A;&#x7B2C;$i$&#x5F20;&#x56FE;&#x7247;&#x5BF9;&#x5E94;&#x7684;&#x7C7B;&#x522B;&#x6807;&#x7B7E;&#xFF08;<code>labels[i]=0,1,2</code>&#xFF09;&#x3002;&#x5047;&#x8BBE;&#x6BCF;&#x5F20;&#x56FE;&#x7247;&#x7684;&#x957F;&#x548C;&#x5BBD;&#x90FD;&#x4E3A;2&#x4E2A;&#x50CF;&#x7D20;&#xFF0C;&#x6709;3&#x4E2A;&#x901A;&#x9053;&#xFF08;$\color{red}{R}\color{green}{G}\color{blue}{B}$&#xFF09;&#x3002;</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line">imgs = np.random.rand(<span class="number">10</span>, <span class="number">2</span>, <span class="number">2</span>, <span class="number">3</span>)</span><br><span class="line">labels = np.random.randint(<span class="number">0</span>, <span class="number">3</span>, size=<span class="number">10</span>)</span><br><span class="line">img_test = np.random.rand(<span class="number">2</span>, <span class="number">2</span>, <span class="number">2</span>, <span class="number">3</span>)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;&#x524D;2&#x5F20;&#x56FE;&#x7247;&#x662F;&#xFF1A;\n&quot;</span>, imgs[:<span class="number">2</span>])</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;\n&#x6240;&#x6709;&#x56FE;&#x7247;&#x7684;&#x6807;&#x7B7E;&#x662F;&#xFF1A;\n&quot;</span>, labels)</span><br></pre></td></tr></table></figure>
<pre><code>&#x524D;2&#x5F20;&#x56FE;&#x7247;&#x662F;&#xFF1A;
 [[[[0.44610755 0.50850947 0.06605838]  // &#x8FD9;&#x4E00;&#x884C;&#x4EE3;&#x8868;&#x7B2C;&#x4E00;&#x5F20;&#x56FE;&#x7247;&#x5DE6;&#x4E0A;&#x89D2;&#x50CF;&#x7D20;&#x7684;RGB&#x6570;&#x503C;
   [0.46162918 0.19888073 0.57064383]]

  [[0.98715871 0.42333655 0.69537932]
   [0.23180493 0.3750864  0.04938028]]]


 [[[0.07719142 0.32079032 0.66292269]
   [0.00509739 0.41437673 0.91452219]]

  [[0.60394883 0.72229577 0.3316002 ]
   [0.59225726 0.54067025 0.19174882]]]]

&#x6240;&#x6709;&#x56FE;&#x7247;&#x7684;&#x6807;&#x7B7E;&#x662F;&#xFF1A;
 [2 1 0 2 1 2 2 2 1 1]
</code></pre><h4 id="&#x6570;&#x636E;&#x9884;&#x5904;&#x7406;"><a href="#&#x6570;&#x636E;&#x9884;&#x5904;&#x7406;" class="headerlink" title="&#x6570;&#x636E;&#x9884;&#x5904;&#x7406;"></a>&#x6570;&#x636E;&#x9884;&#x5904;&#x7406;</h4><p>&#x4E3A;&#x4E86;&#x65B9;&#x4FBF;&#x540E;&#x7EED;&#x5904;&#x7406;&#xFF0C;&#x5C06;&#x6BCF;&#x5F20;&#x56FE;&#x7247;reshape&#x4E3A;&#x957F;&#x5EA6;&#x4E3A;$2\times2\times3 = 12$&#x7684;&#x5411;&#x91CF;&#xFF0C;&#x5E76;&#x4E14;&#x6DFB;&#x52A0;&#x4E00;&#x4E2A;1&#x5728;&#x77E2;&#x91CF;&#x672B;&#x5C3E;&#x4F5C;&#x4E3A;&#x504F;&#x7F6E;&#x7EF4;&#x5EA6;&#xFF08;bias dimension&#xFF09;&#xFF0C;&#x56E0;&#x6B64;&#x56FE;&#x7247;&#x7A7A;&#x95F4;&#x7684;&#x7EF4;&#x5EA6;$D=12+1=13$&#x3002;&#x8FD9;&#x79CD;&#x5904;&#x7406;&#x65B9;&#x5F0F;&#x7684;&#x5408;&#x7406;&#x6027;&#x5982;&#x4E0B;&#x56FE;&#x6240;&#x793A;&#xFF1A;</p>
<p><img src="/2021/10/14/SVM/wb.jpeg" alt="jpeg"></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">imgs = imgs.reshape(<span class="number">10</span>, -<span class="number">1</span>)</span><br><span class="line">imgs = np.hstack([imgs, np.ones((imgs.shape[<span class="number">0</span>], <span class="number">1</span>))])</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;&#x5904;&#x7406;&#x540E;&#x7684;&#x524D;2&#x5F20;&#x56FE;&#x7247;&#x662F;&#xFF1A;\n&quot;</span>, imgs[:<span class="number">2</span>])</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;\n&#x5171;&#x6709;%d&#x5F20;&#x56FE;&#x7247;&#xFF0C;&#x6BCF;&#x5F20;&#x56FE;&#x7247;&#x7684;&#x7EF4;&#x5EA6;&#x662F;%d&#x3002;\n&quot;</span>%imgs.shape)</span><br></pre></td></tr></table></figure>
<pre><code>&#x5904;&#x7406;&#x540E;&#x7684;&#x524D;2&#x5F20;&#x56FE;&#x7247;&#x662F;&#xFF1A;
 [[0.44610755 0.50850947 0.06605838 0.46162918 0.19888073 0.57064383
  0.98715871 0.42333655 0.69537932 0.23180493 0.3750864  0.04938028
  1.        ]
 [0.07719142 0.32079032 0.66292269 0.00509739 0.41437673 0.91452219
  0.60394883 0.72229577 0.3316002  0.59225726 0.54067025 0.19174882
  1.        ]]

&#x5171;&#x6709;10&#x5F20;&#x56FE;&#x7247;&#xFF0C;&#x6BCF;&#x5F20;&#x56FE;&#x7247;&#x7684;&#x7EF4;&#x5EA6;&#x662F;13&#x3002;
</code></pre><p>&#x4E3A;&#x4E86;&#x5BF9;&#x56FE;&#x7247;&#x505A;&#x5206;&#x7C7B;&#xFF0C;&#x7EBF;&#x6027;&#x5206;&#x7C7B;&#x5668;&#x7684;&#x60F3;&#x6CD5;&#x662F;&#x7528;&#x4E00;&#x4E2A;$13\times 3$&#x7EF4;&#x7684;&#x7EBF;&#x6027;&#x53D8;&#x6362;$W$&#x5C06;&#x56FE;&#x7247;&#x5411;&#x91CF;$X_i$&#x4ECE;&#x56FE;&#x7247;&#x7A7A;&#x95F4;&#xFF08;13&#x7EF4;&#xFF09;&#x6620;&#x5C04;&#x5230;&#x7C7B;&#x522B;&#x7A7A;&#x95F4;&#xFF08;3&#x7EF4;&#xFF09;&#xFF0C;&#x82E5;&#x5C06;&#x7EBF;&#x6027;&#x53D8;&#x6362;&#x540E;&#x7684;&#x5411;&#x91CF;&#x8BB0;&#x4F5C;$S_i$&#xFF0C;&#x5219;$S_i$&#x5728;&#x67D0;&#x4E00;&#x7EF4;&#x5EA6;$c$&#x4E0A;&#x7684;&#x5206;&#x91CF;$S_{ic}$&#x5373;&#x4E3A;&#x7B2C;$i$&#x5F20;&#x56FE;&#x7247;&#x5728;&#x7B2C;$c$&#x7C7B;&#x4E0A;&#x7684;&#x5F97;&#x5206;&#x3002;</p>
<h3 id="&#x635F;&#x5931;&#x51FD;&#x6570;"><a href="#&#x635F;&#x5931;&#x51FD;&#x6570;" class="headerlink" title="&#x635F;&#x5931;&#x51FD;&#x6570;"></a>&#x635F;&#x5931;&#x51FD;&#x6570;</h3><p>&#x4E00;&#x4E2A;&#x6734;&#x7D20;&#x7684;&#x60F3;&#x6CD5;&#x662F;&#xFF0C;&#x5F97;&#x5206;$S_{ic}$&#x8D8A;&#x9AD8;&#xFF0C;&#x5C31;&#x4EE3;&#x8868;&#x7740;&#x7B2C;$i$&#x5F20;&#x56FE;&#x7247;&#x8D8A;&#x5E94;&#x8BE5;&#x88AB;&#x5206;&#x5728;&#x7B2C;$c$&#x7C7B;&#x3002;&#x6211;&#x4EEC;&#x5DF2;&#x77E5;&#x7B2C;$i$&#x5F20;&#x56FE;&#x7247;&#x7684;&#x5206;&#x7C7B;$y_i$&#xFF0C;&#x56E0;&#x6B64;&#x53EF;&#x4EE5;&#x5C06;$S_{iy_i}$&#x548C;$S_{ic}(c\neq y_i)$&#x4F5C;&#x5DEE;&#x8FDB;&#x884C;&#x6BD4;&#x8F83;&#xFF0C;&#x5E76;&#x4E14;&#x8BBE;&#x5B9A;&#x4E00;&#x4E2A;&#x6240;&#x8C13;&#x7684;&#x201C;&#x5B89;&#x5168;&#x8DDD;&#x79BB;&#x201D;$\Delta$&#x2014;&#x2014;&#x5F53;$S_{iy_i} - S_{ic}(c\neq y_i) \ge \Delta$&#x65F6;&#xFF0C;&#x6211;&#x4EEC;&#x5C31;&#x53EF;&#x4EE5;&#x8BA4;&#x4E3A;&#x76F8;&#x6BD4;&#x4E8E;&#x7B2C;$c$&#x7C7B;&#xFF0C;&#x7B2C;$i$&#x5F20;&#x56FE;&#x7247;&#x5DF2;&#x7ECF;&#x88AB;&#x8DB3;&#x591F;&#x597D;&#x5730;&#x5206;&#x5728;&#x4E86;&#x7B2C;$y_i$&#x7C7B;&#xFF0C;&#x5373;&#x5BF9;&#x7B2C;$i$&#x5F20;&#x56FE;&#x7247;&#x7684;&#x5206;&#x7C7B;&#x201C;&#x8DB3;&#x591F;&#x6B63;&#x786E;&#x201D;&#x3002;&#x56E0;&#x6B64;&#x53EF;&#x4EE5;&#x5199;&#x51FA;&#x7B2C;$i$&#x5F20;&#x56FE;&#x7247;&#x7684;&#x635F;&#x5931;&#x51FD;&#x6570;&#xFF1A;</p>
<p>\begin{equation}<br>L_i = \sum_{c\neq y_i}\max(0,\ S_{ic} - S_{iy_i} + \Delta) = \sum_{c\neq y_i}\max[0,\ (W^T)_c \cdot X_i - (W^T)_{y_i} \cdot X_i + \Delta]<br>\end{equation}</p>
<p>&#x6839;&#x636E;&#x4E0A;&#x8FF0;&#x5B9A;&#x4E49;&#x548C;&#x8FD0;&#x7B97;&#xFF0C;$W$&#x7684;&#x7B2C;$c$&#x5217;&#x5BF9;&#x5E94;&#x7740;&#x5206;&#x5230;&#x7B2C;$c$&#x7C7B;&#x7684;&#x6743;&#x91CD;&#xFF0C;$S_{ic} = (W^T)_c \cdot X_i$&#xFF0C;&#x5373;$W_{nc}$&#x672C;&#x8D28;&#x4E0A;&#x4EE3;&#x8868;&#x7740;&#x4EFB;&#x4F55;&#x4E00;&#x5F20;&#x56FE;&#x7247;&#x7684;&#x7B2C;$n$&#x4E2A;&#x50CF;&#x7D20;&#x70B9;&#x5BF9;&#x4E8E;&#x5C06;&#x8FD9;&#x5F20;&#x56FE;&#x7247;&#x5206;&#x4E3A;&#x7B2C;$c$&#x7C7B;&#x7684;&#x8D21;&#x732E;&#xFF08;&#x6743;&#x91CD;&#xFF09;&#x3002;</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># &#x968F;&#x673A;&#x521D;&#x59CB;&#x5316;&#x4E00;&#x4E2A;&#x6743;&#x91CD;&#x77E9;&#x9635;W</span></span><br><span class="line">W = np.random.randn(<span class="number">13</span>, <span class="number">3</span>)</span><br><span class="line">S = np.dot(imgs, W)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;W&#x662F;&#x4E00;&#x4E2A;%d*%d&#x7EF4;&#x77E9;&#x9635;\n&quot;</span>%W.shape, W)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;\nS&#x662F;&#x4E00;&#x4E2A;%d*%d&#x7EF4;&#x77E9;&#x9635;\n&quot;</span>%S.shape, S)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;\n&#x6240;&#x6709;&#x56FE;&#x7247;&#x7684;&#x6807;&#x7B7E;&#x662F;&#xFF1A;\n&quot;</span>, labels)</span><br></pre></td></tr></table></figure>
<pre><code>W&#x662F;&#x4E00;&#x4E2A;13*3&#x7EF4;&#x77E9;&#x9635;
 [[ 0.38922976 -1.5476645   0.29391136]
 [ 1.88278845 -2.03980096  0.87708064]
 [ 0.6370439   1.20157172 -1.41819386]
 [-0.72895385  1.54486262 -0.13406272]
 [-1.0054797  -0.87397261  0.97254715]
 [ 1.4744262  -0.09882989  0.06366316]
 [ 1.07822844  1.34769706  1.57454748]
 [ 0.11163696 -1.39929385 -0.46959958]
 [ 1.61945353 -0.25428393 -1.06335361]
 [ 0.19482573  0.42195421  0.34891287]
 [ 1.51212699 -2.70581067 -0.03097324]
 [ 0.78342346  0.2109576   0.17596721]
 [-0.38857257  0.52356264 -0.07280062]]

S&#x662F;&#x4E00;&#x4E2A;10*3&#x7EF4;&#x77E9;&#x9635;
 [[ 3.9782615  -0.98729542  1.27254517]
 [ 3.94780649 -1.3520439   0.23442124]
 [ 4.75273802 -0.32327394 -1.08898947]
 [ 5.24844741 -1.91383568  1.44559066]
 [ 4.39185774 -0.42945789  1.34646468]
 [ 4.64370727 -2.85630203  0.36418856]
 [ 3.67487502 -2.61126693 -0.97194457]
 [ 2.64962175 -1.43415149  0.23078515]
 [ 2.69766682 -1.9465858  -0.15695445]
 [ 1.8519816   0.27556944  0.24302435]]

 &#x6240;&#x6709;&#x56FE;&#x7247;&#x7684;&#x6807;&#x7B7E;&#x662F;&#xFF1A;
 [2 1 0 2 1 2 2 2 1 1]
</code></pre><p>&#x4E3A;&#x4E86;&#x6C42;&#x51FA;&#x7B2C;$i$&#x5F20;&#x56FE;&#x7247;&#x7684;&#x635F;&#x5931;&#x51FD;&#x6570;$L_i$&#xFF0C;&#x9996;&#x5148;&#x4ECE;&#x5F97;&#x5206;&#x77E9;&#x9635;$S$&#x4E2D;&#x6311;&#x51FA;&#x6240;&#x6709;&#x771F;&#x5B9E;&#x5206;&#x7C7B;&#x7684;&#x5F97;&#x5206;&#xFF0C;&#x5C06;&#x5176;&#x5EF6;&#x5C55;&#x4E3A;&#x4E0E;$S$&#x76F8;&#x540C;&#x7684;&#x7EF4;&#x5EA6;&#xFF0C;&#x4E0E;&#x5F97;&#x5206;&#x77E9;&#x9635;$S$&#x4F5C;&#x5DEE;&#x540E;&#x52A0;&#x4E0A;$\Delta$&#x5373;&#x53EF;&#x5F97;&#x5230;&#xFF08;&#x672A;&#x7ECF;&#x5904;&#x7406;&#x7684;&#xFF09;$L$&#x77E9;&#x9635;&#xFF1A;</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">correct_scores = S[<span class="built_in">range</span>(S.shape[<span class="number">0</span>]), labels].reshape(-<span class="number">1</span>,<span class="number">1</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;correct_scores:\n&quot;</span>, correct_scores)</span><br><span class="line"></span><br><span class="line">correct_scores = np.hstack([correct_scores] * S.shape[<span class="number">1</span>])</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;\ncorre_scores after broadcasting:\n&quot;</span>, correct_scores)</span><br><span class="line"></span><br><span class="line">Delta = <span class="number">1</span>  <span class="comment"># &#x4E00;&#x4E2A;&#x76F8;&#x5BF9;&#x4EFB;&#x610F;&#x9009;&#x62E9;&#x7684;&#x201C;&#x5B89;&#x5168;&#x8DDD;&#x79BB;&#x201D;</span></span><br><span class="line">L = S - correct_scores + Delta</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;\nL:\n&quot;</span>, L)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;\nlabels:&quot;</span>, labels)</span><br></pre></td></tr></table></figure>
<pre><code>correct_scores:
 [[ 1.27254517]
 [-1.3520439 ]
 [ 4.75273802]
 [ 1.44559066]
 [-0.42945789]
 [ 0.36418856]
 [-0.97194457]
 [ 0.23078515]
 [-1.9465858 ]
 [ 0.27556944]]

correct_scores after broadcasting:
 [[ 1.27254517  1.27254517  1.27254517]
 [-1.3520439  -1.3520439  -1.3520439 ]
 [ 4.75273802  4.75273802  4.75273802]
 [ 1.44559066  1.44559066  1.44559066]
 [-0.42945789 -0.42945789 -0.42945789]
 [ 0.36418856  0.36418856  0.36418856]
 [-0.97194457 -0.97194457 -0.97194457]
 [ 0.23078515  0.23078515  0.23078515]
 [-1.9465858  -1.9465858  -1.9465858 ]
 [ 0.27556944  0.27556944  0.27556944]]

L:
 [[ 3.70571632 -1.25984059  1.        ]
 [ 6.2998504   1.          2.58646514]
 [ 1.         -4.07601195 -4.84172749]
 [ 4.80285675 -2.35942634  1.        ]
 [ 5.82131563  1.          2.77592256]
 [ 5.27951872 -2.22049058  1.        ]
 [ 5.64681958 -0.63932236  1.        ]
 [ 3.41883661 -0.66493664  1.        ]
 [ 5.64425262  1.          2.78963135]
 [ 2.57641216  1.          0.96745491]]

labels: [2 1 0 2 1 2 2 2 1 1]
</code></pre><p>&#x53EF;&#x4EE5;&#x770B;&#x5230;$L$&#x77E9;&#x9635;&#x4E2D;&#x51FA;&#x73B0;&#x4E86;&#x5F88;&#x591A;$1$&#xFF0C;&#x8FD9;&#x4E9B;$1$&#x51FA;&#x73B0;&#x7684;&#x4F4D;&#x7F6E;&#x6B63;&#x662F;&#x6BCF;&#x5F20;&#x56FE;&#x7247;&#x7684;&#x6B63;&#x786E;&#x5206;&#x7C7B;&#x4F4D;&#x7F6E;&#xFF0C;&#x5373;$L_{iy_i} = \Delta (= 1)$&#x3002;&#x6839;&#x636E;$L_i$&#x7684;&#x5B9A;&#x4E49;&#xFF0C;&#x6211;&#x4EEC;&#x8FD8;&#x9700;&#x8981;&#x505A;&#x4E24;&#x6B65;&#x64CD;&#x4F5C;&#xFF1A;</p>
<ol>
<li>&#x5C06;&#x6BCF;&#x4E2A;&#x56FE;&#x7247;&#x7684;&#x6B63;&#x786E;&#x5206;&#x7C7B;&#x4F4D;&#x7F6E;&#x7F6E;&#x96F6;&#xFF0C;&#x56E0;&#x4E3A;&#x516C;&#x5F0F;&#x4E2D;&#x5E76;&#x4E0D;&#x5305;&#x542B;&#x8FD9;&#x4E00;&#x9879;&#xFF1B;</li>
<li>&#x5C06;&#x5C0F;&#x4E8E;&#x96F6;&#x7684;&#x4F4D;&#x7F6E;&#x7F6E;&#x96F6;&#xFF0C;&#x56E0;&#x4E3A;&#x516C;&#x5F0F;&#x4E2D;&#x7684;$\max(0, blabla)$&#x4FDD;&#x8BC1;&#x4E86;&#x8FD9;&#x4E00;&#x70B9;&#x3002;</li>
</ol>
<p>&#x6211;&#x4EEC;&#x4F7F;&#x7528;&#x4E00;&#x4E2A;&#x8499;&#x7248;<code>mask</code>&#x6765;&#x5B9E;&#x73B0;&#x8FD9;&#x4E24;&#x6B65;&#x5904;&#x7406;&#xFF0C;&#x800C;&#x4E0D;&#x76F4;&#x63A5;&#x4FEE;&#x6539;$L$&#x672C;&#x8EAB;&#x3002;<br><div class="note info no-icon"><p>&#x6CE8;&#xFF1A;&#x4F7F;&#x7528;<code>mask</code>&#x7684;&#x597D;&#x5904;&#x5C06;&#x5728;&#x8BA1;&#x7B97;&#x68AF;&#x5EA6;<code>dW</code>&#x65F6;&#x5F97;&#x4EE5;&#x4F53;&#x73B0;&#x3002;</p>
</div></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">mask = np.ones(L.shape)</span><br><span class="line">mask[<span class="built_in">range</span>(L.shape[<span class="number">0</span>]), labels] = <span class="number">0</span>  <span class="comment"># &#x6B63;&#x786E;&#x5206;&#x7C7B;&#x4F4D;&#x7F6E;&#x7F6E;&#x96F6;</span></span><br><span class="line">mask[L &lt; <span class="number">0</span>] = <span class="number">0</span>  <span class="comment"># &#x5C0F;&#x4E8E;&#x96F6;&#x7684;&#x4F4D;&#x7F6E;&#x7F6E;&#x96F6;</span></span><br><span class="line">L *= mask</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;mask = \n&quot;</span>, mask)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;\nL = \n&quot;</span>, L)</span><br></pre></td></tr></table></figure>
<pre><code>mask = 
 [[1. 0. 0.]
 [1. 0. 1.]
 [0. 0. 0.]
 [1. 0. 0.]
 [1. 0. 1.]
 [1. 0. 0.]
 [1. 0. 0.]
 [1. 0. 0.]
 [1. 0. 1.]
 [1. 0. 1.]]

L = 
 [[ 3.70571632 -0.          0.        ]
 [ 6.2998504   0.          2.58646514]
 [ 0.         -0.         -0.        ]
 [ 4.80285675 -0.          0.        ]
 [ 5.82131563  0.          2.77592256]
 [ 5.27951872 -0.          0.        ]
 [ 5.64681958 -0.          0.        ]
 [ 3.41883661 -0.          0.        ]
 [ 5.64425262  0.          2.78963135]
 [ 2.57641216  0.          0.96745491]]
</code></pre><p>&#x7ECF;&#x8FC7;&#x5904;&#x7406;&#x7684;$L$&#x4E2D;&#x4ECD;&#x4E0D;&#x4E3A;0&#x7684;&#x4F4D;&#x7F6E;&#x5B9E;&#x9645;&#x4E0A;&#x5C31;&#x662F;&#x5206;&#x7C7B;&#x7ED3;&#x679C;&#x4E0D;&#x591F;&#x597D;&#x7684;&#x4F4D;&#x7F6E;&#xFF0C;&#x6570;&#x5B57;&#x8D8A;&#x5927;&#x5C31;&#x4EE3;&#x8868;&#x5206;&#x7C7B;&#x7ED3;&#x679C;&#x8D8A;&#x5DEE;&#xFF1B;&#x53CD;&#x4E4B;&#xFF0C;&#x82E5;&#x7B2C;$i$&#x884C;&#x7684;&#x6570;&#x5B57;&#x5168;&#x4E3A;0&#xFF0C;&#x5219;&#x4EE3;&#x8868;&#x5F53;&#x524D;&#x7684;$W$&#x4E0D;&#x4EC5;&#x5BF9;&#x4E8E;&#x7B2C;$i$&#x5F20;&#x56FE;&#x7247;&#x7684;&#x5206;&#x7C7B;&#x662F;&#x6B63;&#x786E;&#x7684;&#xFF0C;&#x5E76;&#x4E14;&#x5C06;&#x5176;&#x4ED6;&#x56FE;&#x7247;&#x62D2;&#x4E8E;&#x201C;&#x5B89;&#x5168;&#x8DDD;&#x79BB;&#x201D;&#x4EE5;&#x5916;&#x3002;&#x201C;&#x5B89;&#x5168;&#x8DDD;&#x79BB;&#x201D;&#x7684;&#x6982;&#x5FF5;&#x53EF;&#x4EE5;&#x7528;&#x4E0B;&#x56FE;&#x5F62;&#x8C61;&#x5730;&#x5C55;&#x793A;&#xFF1A;</p>
<p><img src="/2021/10/14/SVM/margin.jpg" alt="jpg"></p>
<p>&#x73B0;&#x5728;&#xFF0C;$L$&#x7684;&#x7B2C;$i$&#x884C;&#x6570;&#x5B57;&#x4E4B;&#x548C;&#x5373;&#x4E3A;$L_i$&#xFF0C;&#x77E9;&#x9635;&#x4E2D;&#x6240;&#x6709;&#x6570;&#x5B57;&#x4E4B;&#x548C;&#x5373;&#x4E3A;&#x603B;&#x7684;&#x635F;&#x5931;&#x51FD;&#x6570;$L$&#xFF1A;</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">L = np.<span class="built_in">sum</span>(L, axis=<span class="number">1</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;L = &quot;</span>, L)</span><br><span class="line"></span><br><span class="line">loss = np.<span class="built_in">sum</span>(L) / <span class="built_in">len</span>(L)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;\nloss = &quot;</span>, loss)</span><br></pre></td></tr></table></figure>
<pre><code>L = 
[3.70571632 8.88631554 0.         4.80285675 8.59723819 5.27951872
 5.64681958 3.41883661 8.43388397 3.54386707]

loss = 5.231505276181342
</code></pre><p>&#x8BD5;&#x60F3;&#xFF0C;&#x5982;&#x679C;&#x5C06;$W$&#x6269;&#x5927;$\alpha$&#x500D;&#xFF0C;&#x90A3;&#x4E48;&#x9664;&#x4E86;&#x635F;&#x5931;&#x51FD;&#x6570;&#x4E5F;&#x76F8;&#x5E94;&#x5730;&#x6269;&#x5927;$\alpha$&#x500D;&#x4EE5;&#x5916;&#xFF0C;&#x4E0D;&#x4F1A;&#x518D;&#x6709;&#x5176;&#x4ED6;&#x5F71;&#x54CD;&#x3002;&#x7531;&#x6B64;&#xFF0C;&#x6211;&#x4EEC;&#x53EF;&#x4EE5;&#x4E3A;&#x635F;&#x5931;&#x51FD;&#x6570;&#x6DFB;&#x52A0;&#x6B63;&#x5219;&#x60E9;&#x7F5A;(regularization penalty)&#xFF0C;&#x8FD9;&#x5176;&#x4E2D;&#x8574;&#x542B;&#x7740;&#x5965;&#x5361;&#x59C6;&#x5243;&#x5200;&#x7684;&#x601D;&#x60F3;&#xFF0C;&#x5373;&#x9F13;&#x52B1;&#x7B80;&#x5316;&#x3001;&#x9F13;&#x52B1;&#x7A00;&#x758F;&#x3001;&#x53CD;&#x5BF9;&#x4E0D;&#x5FC5;&#x8981;&#x7684;&#x590D;&#x6742;&#xFF0C;&#x8FD9;&#x540C;&#x65F6;&#x4E5F;&#x80FD;&#x4E00;&#x5B9A;&#x7A0B;&#x5EA6;&#x4E0A;&#x9632;&#x6B62;&#x8FC7;&#x62DF;&#x5408;&#x3002;&#x516C;&#x5F0F;&#x4E2D;&#x7684;$\lambda$&#x662F;&#x6B63;&#x5219;&#x60E9;&#x7F5A;&#x7684;&#x529B;&#x5EA6;&#xFF0C;$\sum_k\sum_l W_{k,l}^2$&#x662F;&#x5F53;&#x524D;&#x7684;$W$&#x4E0E;&#x96F6;&#x77E9;&#x9635;&#x7684;Frobenius&#x8303;&#x6570;&#xFF0C;&#x7528;&#x6765;&#x5B9A;&#x91CF;&#x8BC4;&#x4F30;&#x4E24;&#x8005;&#x7684;&#x5DEE;&#x5F02;&#x3002;</p>
<p>&#x52A0;&#x5165;&#x6B63;&#x5219;&#x60E9;&#x7F5A;&#x7684;&#x603B;&#x635F;&#x5931;&#x51FD;&#x6570;&#x4E3A;&#xFF1A;</p>
<p>\begin{equation}<br>L = \frac{1}{N} \sum_i \sum_{c\neq y_i}  \max\left[0,\ (X_i\cdot W)_c - (X_i\cdot W)_{y_i} + \Delta) \right] + \lambda \sum_k\sum_l W_{k,l}^2<br>\end{equation}</p>
<p>&#x5C06;&#x4E0A;&#x9762;&#x7684;&#x6B65;&#x9AA4;&#x8FDB;&#x884C;&#x6574;&#x5408;&#x5C31;&#x53EF;&#x4EE5;&#x5199;&#x51FA;&#x5B8C;&#x6574;&#x7684;<code>svm_loss</code>&#x51FD;&#x6570;&#xFF1A;</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">svm_loss</span>(<span class="params">W, X, y, delta, reg</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    SVM loss function, vectorized implementation.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Input images have dimension D, there are C classes, </span></span><br><span class="line"><span class="string">    and we operate on N examples.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Inputs:</span></span><br><span class="line"><span class="string">    - W: A numpy array of shape (D, C) containing weights.</span></span><br><span class="line"><span class="string">    - X: A numpy array of shape (N, D) containing a minibatch of data.</span></span><br><span class="line"><span class="string">    - y: A numpy array of shape (N,) containing training labels; y[i] = c means</span></span><br><span class="line"><span class="string">      that X[i] has label c, where 0 &lt;= c &lt; C.</span></span><br><span class="line"><span class="string">    - delta: (float) classification safe margin</span></span><br><span class="line"><span class="string">    - reg: (float) regularization strength</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Return:</span></span><br><span class="line"><span class="string">    - loss as single float</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">    S = np.dot(X, W)</span><br><span class="line">    correct_scores = S[<span class="built_in">range</span>(S.shape[<span class="number">0</span>]), y].reshape(-<span class="number">1</span>,<span class="number">1</span>)</span><br><span class="line">    correct_scores = np.hstack([correct_scores] * S.shape[<span class="number">1</span>])</span><br><span class="line"></span><br><span class="line">    L = S - correct_scores + delta</span><br><span class="line"></span><br><span class="line">    mask = np.ones(L.shape)</span><br><span class="line">    mask[<span class="built_in">range</span>(L.shape[<span class="number">0</span>]), y] = <span class="number">0</span>  <span class="comment"># &#x6B63;&#x786E;&#x5206;&#x7C7B;&#x4F4D;&#x7F6E;&#x7F6E;&#x96F6;</span></span><br><span class="line">    mask[L &lt; <span class="number">0</span>] = <span class="number">0</span>                 <span class="comment"># &#x5C0F;&#x4E8E;&#x96F6;&#x7684;&#x4F4D;&#x7F6E;&#x7F6E;&#x96F6;</span></span><br><span class="line">    L *= mask</span><br><span class="line"></span><br><span class="line">    loss = np.<span class="built_in">sum</span>(L) / <span class="built_in">len</span>(L)</span><br><span class="line">    loss += reg * np.<span class="built_in">sum</span>(W * W)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> loss</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">loss = svm_loss(W, imgs, labels, delta=<span class="number">1</span>, reg=<span class="number">0.005</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;loss = &quot;</span>, loss)</span><br></pre></td></tr></table></figure>
<pre><code>loss =  5.458238730215393
</code></pre><h3 id="&#x68AF;&#x5EA6;"><a href="#&#x68AF;&#x5EA6;" class="headerlink" title="&#x68AF;&#x5EA6;"></a>&#x68AF;&#x5EA6;</h3><h4 id="&#x68AF;&#x5EA6;&#x7684;&#x8BE6;&#x7EC6;&#x63A8;&#x5BFC;"><a href="#&#x68AF;&#x5EA6;&#x7684;&#x8BE6;&#x7EC6;&#x63A8;&#x5BFC;" class="headerlink" title="&#x68AF;&#x5EA6;&#x7684;&#x8BE6;&#x7EC6;&#x63A8;&#x5BFC;"></a>&#x68AF;&#x5EA6;&#x7684;&#x8BE6;&#x7EC6;&#x63A8;&#x5BFC;</h4><p>&#x73B0;&#x5728;&#x6765;&#x8BA1;&#x7B97;&#x68AF;&#x5EA6;&#x77E9;&#x9635;<code>dW</code>&#x3002;&#x6839;&#x636E;&#x68AF;&#x5EA6;&#x7684;&#x5B9A;&#x4E49;&#xFF0C;<code>dW</code>&#x5177;&#x6709;&#x5982;&#x4E0B;&#x539F;&#x59CB;&#x5F62;&#x5F0F;&#xFF1A;</p>
<p>\begin{equation}<br>    \nabla_{W} L<br>    :=<br>    \begin{bmatrix}<br>        \frac{dL}{dW_{11}} &amp; \frac{dL}{dW_{12}} &amp; \cdots &amp; \frac{dL}{dW_{1C}} \\<br>        \frac{dL}{dW_{21}} &amp; \frac{dL}{dW_{22}} &amp; \cdots &amp; \frac{dL}{dW_{2C}} \\<br>        \vdots &amp; \vdots &amp; \ddots &amp; \vdots \\<br>        \frac{dL}{dW_{D1}} &amp; \frac{dL}{dW_{D2}} &amp; \cdots &amp; \frac{dL}{dW_{DC}}<br>    \end{bmatrix}<br>\end{equation}</p>
<p>&#x5728;&#x672C;&#x4F8B;&#x4E2D;&#xFF0C;&#x56FE;&#x7247;&#x6570;&#x91CF;&#x4E3A;$N=10$&#xFF0C;&#x56FE;&#x7247;&#x7A7A;&#x95F4;&#x7EF4;&#x5EA6;$D=13$&#xFF0C;&#x7C7B;&#x522B;&#x6570;$C=3$&#x3002;&#x7531;&#x4E8E;&#x4E0A;&#x9762;&#x7684;&#x6570;&#x636E;&#x662F;&#x968F;&#x673A;&#x751F;&#x6210;&#x7684;&#xFF0C;&#x6545;&#x6BCF;&#x6B21;&#x8FD0;&#x884C;&#x7A0B;&#x5E8F;&#x6240;&#x5F97;&#x7684;<code>mask</code>&#x4E5F;&#x4E0D;&#x540C;&#x3002;&#x73B0;&#x5728;&#x4EE5;&#x4E0B;&#x9762;&#x7ED9;&#x5B9A;&#x7684;<code>mask</code>&#x4E3A;&#x4F8B;&#x505A;&#x4E00;&#x4E9B;&#x5206;&#x6790;&#xFF0C;&#x4E3A;&#x540E;&#x7EED;&#x7684;&#x5411;&#x91CF;&#x5316;&#x8BA1;&#x7B97;<code>dW</code>&#x634B;&#x6E05;&#x601D;&#x8DEF;&#x3002;&#x4E3A;&#x4E86;&#x7B80;&#x660E;&#xFF0C;&#x53EA;&#x5206;&#x6790;<code>mask</code>&#x7684;&#x524D;&#x4E24;&#x884C;&#x3002;</p>
<p>\begin{equation}<br>    mask<br>    =<br>    \begin{bmatrix}<br>        \color{red}{0} &amp; 1 &amp; 1 \\<br>        1 &amp; \color{green}{0} &amp; \color{red}{0} \\<br>        \vdots &amp; \vdots &amp; \vdots \\<br>    \end{bmatrix}<br>\end{equation}</p>
<p>&#x8FD9;&#x91CC;&#x7EA2;&#x8272;&#x7684;$\color{red}{0}$&#x4EE3;&#x8868;&#x56E0;&#x4E3A;&#x5206;&#x7C7B;&#x6B63;&#x786E;&#x800C;&#x7F6E;&#x96F6;&#xFF0C;&#x7EFF;&#x8272;&#x7684;$\color{green}{0}$&#x4EE3;&#x8868;&#x56E0;&#x4E3A;&#x5C0F;&#x4E8E;&#x96F6;&#x800C;&#x7F6E;&#x96F6;&#xFF08;&#x53C2;&#x8003;&#x4E0A;&#x9762;&#x7684;&#x5206;&#x6790;&#xFF09;&#x3002;&#x73B0;&#x5728;&#x4EE5;&#x8FD9;&#x4E2A;<code>mask</code>&#x4E3A;&#x4F8B;&#x5206;&#x6790;&#x68AF;&#x5EA6;<code>dW</code>&#x4E2D;&#x7684;&#x51E0;&#x4E2A;&#x5143;&#x7D20;&#x3002;&#x5148;&#x5199;&#x51FA;$L_1$&#x6700;&#x7EC8;&#x7684;&#x8868;&#x8FBE;&#x5F0F;&#xFF1A;</p>
<p>\begin{equation}<br>L_1 = (S_{12} - S_{11} + \Delta) + (S_{13} - S_{11} + \Delta)<br>\end{equation}</p>
<p>&#x5229;&#x7528;&#x94FE;&#x5F0F;&#x6CD5;&#x5219;&#x53EF;&#x4EE5;&#x5F97;&#x5230;</p>
<p>\begin{equation}<br>\frac{\partial L_1}{\partial W_{11}} = \sum_c \frac{\partial L_1}{\partial S_{1c}} \frac{\partial S_{1c}}{\partial W_{11}} = (-2) X_{11}<br>\end{equation}</p>
<p>&#x540C;&#x7406;&#xFF0C;<br>\begin{equation}<br>\frac{\partial L_1}{\partial W_{n1}} = \sum_c \frac{\partial L_1}{\partial S_{1c}} \frac{\partial S_{1c}}{\partial W_{n1}} = (-2) X_{1n}<br>\end{equation}</p>
<p>&#x8FD9;&#x91CC;&#x7684;&#x7CFB;&#x6570;$-2$&#x6765;&#x81EA;&#x4E8E;<code>mask</code>&#x4E2D;&#x7B2C;&#x4E00;&#x884C;&#x6709;2&#x4E2A;1&#xFF0C;&#x5176;&#x4E2D;&#x7684;&#x8D1F;&#x53F7;&#x662F;&#x56E0;&#x4E3A;$W$&#x7684;&#x7B2C;&#x4E8C;&#x4E2A;&#x4E0B;&#x6807;1&#x6B63;&#x597D;&#x5BF9;&#x5E94;&#x7B2C;&#x4E00;&#x5F20;&#x56FE;&#x7247;&#x7684;&#x6B63;&#x786E;&#x5206;&#x7C7B;&#x6807;&#x7B7E;$y_1 = 0$&#xFF08;&#x6CE8;&#xFF1A;&#x6B64;&#x5904;&#x7684;&#x56FE;&#x7247;&#x88AB;&#x5206;&#x4E3A;&#x4E09;&#x7C7B;&#xFF0C;&#x5373;$\{1,2,3\}$&#xFF0C;&#x5BF9;&#x5E94;&#x5206;&#x7C7B;&#x7684;&#x6807;&#x7B7E;$\{0,1,2\}$&#xFF09;&#xFF0C;&#x4ECE;&#x800C;$S_{11}$&#x51FA;&#x73B0;&#x5728;&#x516C;&#x5F0F;&#x91CC;&#x7684;&#x8D1F;&#x53F7;&#x540E;&#x3002;&#x56E0;&#x6B64;$L_1$&#x5BF9;<code>dW</code>&#x7684;&#x7B2C;1&#x5217;&#x8D21;&#x732E;&#x4E86;$-2$&#x4E2A;$X_1$&#x3002;</p>
<p>&#x518D;&#x770B;&#x53E6;&#x4E00;&#x4E2A;&#x5143;&#x7D20;&#xFF1A;</p>
<p>\begin{equation}<br>\frac{\partial L_1}{\partial W_{12}} = \sum_c \frac{\partial L_1}{\partial S_{1c}} \frac{\partial S_{1c}}{\partial W_{12}} = X_{11}<br>\end{equation}</p>
<p>&#x540C;&#x7406;&#xFF0C;<br>\begin{equation}<br>\frac{\partial L_1}{\partial W_{n2}} = \sum_c \frac{\partial L_1}{\partial S_{1c}} \frac{\partial S_{1c}}{\partial W_{n2}} = X_{1n} \ ,\ \frac{\partial L_1}{\partial W_{n3}} = \sum_c \frac{\partial L_1}{\partial S_{1c}} \frac{\partial S_{1c}}{\partial W_{n3}} = X_{1n}<br>\end{equation}</p>
<p>&#x4E0E;&#x4E0A;&#x9762;&#x7684;&#x5206;&#x6790;&#x7C7B;&#x540C;&#xFF0C;$L_1$&#x5BF9;<code>dW</code>&#x7684;&#x7B2C;2&#x5217;&#x548C;&#x7B2C;3&#x5217;&#x5206;&#x522B;&#x8D21;&#x732E;&#x4E86;1&#x4E2A;$X_1$&#x3002;&#x5C06;&#x76EE;&#x524D;&#x5DF2;&#x7ECF;&#x5206;&#x6790;&#x8FC7;&#x7684;&#x90E8;&#x5206;&#x5199;&#x6210;&#x66F4;&#x7B80;&#x7EC3;&#x7684;&#x77E9;&#x9635;&#x5F62;&#x5F0F;&#xFF1A;</p>
<script type="math/tex; mode=display">\begin{equation}
    \nabla_{W} L_1
    =
    \begin{bmatrix}
        X_1 & X_2 & \cdots & X_N
    \end{bmatrix}
    \begin{bmatrix}
        \color{red}{-2} & 1 & 1 \\
        0 & 0 & 0 \\
        \vdots & \vdots & \vdots \\
    \end{bmatrix}
    =
    X^T mask_1
\end{equation}</script><p>&#x63A5;&#x4E0B;&#x6765;&#x5206;&#x6790;$L_2$&#xFF0C;&#x4E0E;$L_1$&#x5B8C;&#x5168;&#x7C7B;&#x540C;&#xFF0C;&#x4F46;&#x7531;&#x4E8E;$L_2 = (S_{21} - S_{23} + \Delta)$&#xFF0C;$L_2$&#x5BF9;<code>dW</code>&#x7684;&#x7B2C;1&#x5217;&#x8D21;&#x732E;&#x4E86;1&#x4E2A;$X_2$&#xFF0C;&#x7B2C;3&#x5217;&#x8D21;&#x732E;&#x4E86;-1&#x4E2A;$X_2$&#xFF0C;&#x5199;&#x8FDB;&#x77E9;&#x9635;&#x5373;&#x4E3A;&#xFF1A;</p>
<p>\begin{equation}<br>    \nabla_{W} L_2<br>    =<br>    \begin{bmatrix}<br>        X_1 &amp; X_2 &amp; \cdots &amp; X_N<br>    \end{bmatrix}<br>    \begin{bmatrix}<br>        0 &amp; 0 &amp; 0 \\<br>        1 &amp; \color{green}{0} &amp; \color{red}{-1} \\<br>        0 &amp; 0 &amp; 0 \\<br>        \vdots &amp; \vdots &amp; \vdots \\<br>    \end{bmatrix}<br>    =<br>    X^T mask_2<br>\end{equation}</p>
<p>&#x5C06;$mask_1,mask_2$&#x4E0E;$mask$&#x5BF9;&#x6BD4;&#x53EF;&#x77E5;&#xFF0C;$mask_i$&#x5C31;&#x662F;&#x5C06;$mask$&#x7684;&#x7B2C;$i$&#x884C;&#x5355;&#x72EC;&#x53D6;&#x51FA;&#xFF0C;&#x5728;&#x7EA2;&#x8272;&#x4F4D;&#x7F6E;&#xFF08;&#x5373;&#x7B2C;$i$&#x5F20;&#x56FE;&#x7247;&#x7684;&#x6B63;&#x786E;&#x5206;&#x7C7B;&#x4F4D;&#x7F6E;&#xFF09;&#x4E0A;&#x586B;&#x5165;$mask$&#x7684;&#x7B2C;$i$&#x884C;&#x4E4B;&#x548C;&#x7684;&#x76F8;&#x53CD;&#x6570;&#x3002;</p>
<p>&#x6700;&#x7EC8;&#x5B8C;&#x6574;&#x7684;<code>dW</code>&#x77E2;&#x91CF;&#x5316;&#x8BA1;&#x7B97;&#x516C;&#x5F0F;&#x4E3A;&#xFF1A;</p>
<p>\begin{equation}<br>    \nabla_{W} L<br>    =<br>    \sum_{i=1}^N \nabla_{W} L_i<br>    =<br>    \begin{bmatrix}<br>        X_1 &amp; X_2 &amp; \cdots &amp; X_N<br>    \end{bmatrix}<br>    \sum_{i=1}^N mask_i<br>    =<br>    X^T<br>    \begin{bmatrix}<br>        \color{red}{-2} &amp; 1 &amp; 1 \\<br>        1 &amp; \color{green}{0} &amp; \color{red}{-1} \\<br>        \vdots &amp; \vdots &amp; \vdots \\<br>    \end{bmatrix}<br>\end{equation}</p>
<p>&#x522B;&#x5FD8;&#x4E86;&#x52A0;&#x5165;&#x6B63;&#x5219;&#x5316;&#x60E9;&#x7F5A;&#xFF1A;</p>
<p>\begin{equation}<br>    \nabla_{W} L<br>    =<br>    \frac{1}{N} X^T<br>    \begin{bmatrix}<br>        \color{red}{-2} &amp; 1 &amp; 1 \\<br>        1 &amp; \color{green}{0} &amp; \color{red}{-1} \\<br>        \vdots &amp; \vdots &amp; \vdots \\<br>    \end{bmatrix} + 2\lambda W<br>\end{equation}</p>
<p>&#x7531;&#x6B64;&#x53EF;&#x4EE5;&#x5199;&#x51FA;&#x5B8C;&#x6574;&#x7684;&#x4EE3;&#x7801;&#xFF1A;</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">svm_loss_grad</span>(<span class="params">W, X, y, delta, reg</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    Structured SVM loss function, vectorized implementation.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Inputs have dimension D, there are C classes, and we operate on minibatches</span></span><br><span class="line"><span class="string">    of N examples.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Inputs:</span></span><br><span class="line"><span class="string">    - W: A numpy array of shape (D, C) containing weights.</span></span><br><span class="line"><span class="string">    - X: A numpy array of shape (N, D) containing a minibatch of data.</span></span><br><span class="line"><span class="string">    - y: A numpy array of shape (N,) containing training labels; y[i] = c means</span></span><br><span class="line"><span class="string">      that X[i] has label c, where 0 &lt;= c &lt; C.</span></span><br><span class="line"><span class="string">    - delta: (float) classification safe margin</span></span><br><span class="line"><span class="string">    - reg: (float) regularization strength</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Returns a tuple of:</span></span><br><span class="line"><span class="string">    - loss as single float</span></span><br><span class="line"><span class="string">    - gradient with respect to weights W; an array of same shape as W</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">    loss = <span class="number">0.0</span></span><br><span class="line">    dW = np.zeros(W.shape)  <span class="comment"># initialize the gradient as zero</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># A vectorized version of the structured SVM loss and gradient.</span></span><br><span class="line">    S = np.dot(X, W)</span><br><span class="line">    correct_scores = S[<span class="built_in">range</span>(S.shape[<span class="number">0</span>]), y].reshape(-<span class="number">1</span>,<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># &#x5229;&#x7528;numpy&#x7684;broadcast&#x7279;&#x6027;&#x53EF;&#x4EE5;&#x7701;&#x7565;&#x4E0B;&#x9762;&#x8FD9;&#x884C;&#x4EE3;&#x7801;</span></span><br><span class="line">    <span class="comment"># correct_scores = np.hstack([correct_scores] * S.shape[1])</span></span><br><span class="line"></span><br><span class="line">    L = S - correct_scores + delta</span><br><span class="line"></span><br><span class="line">    mask = np.ones(L.shape)</span><br><span class="line">    mask[<span class="built_in">range</span>(L.shape[<span class="number">0</span>]), y] = <span class="number">0</span>  <span class="comment"># &#x6B63;&#x786E;&#x5206;&#x7C7B;&#x4F4D;&#x7F6E;&#x7F6E;&#x96F6;</span></span><br><span class="line">    mask[L &lt; <span class="number">0</span>] = <span class="number">0</span>  <span class="comment"># &#x5C0F;&#x4E8E;&#x96F6;&#x7684;&#x4F4D;&#x7F6E;&#x7F6E;&#x96F6;</span></span><br><span class="line">    L *= mask</span><br><span class="line"></span><br><span class="line">    loss += np.<span class="built_in">sum</span>(L) / X.shape[<span class="number">0</span>]</span><br><span class="line">    loss += reg * np.<span class="built_in">sum</span>(W * W)</span><br><span class="line"></span><br><span class="line">    dW += np.dot(X.T, mask) / X.shape[<span class="number">0</span>]  <span class="comment"># D*C = D*N dot N*C</span></span><br><span class="line">    dW += <span class="number">2</span> * reg * W</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> loss, dW</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">loss, dW = svm_loss_grad(W, imgs, labels, delta=<span class="number">1</span>, reg=<span class="number">0.005</span>)</span><br><span class="line">loss, dW</span><br></pre></td></tr></table></figure>
<pre><code>(7.457803289988815,
 array([[-0.06685351, -0.08443495, -0.04007769],
        [-0.03463011, -0.05389141, -0.02550633],
        [-0.03927926, -0.04445769, -0.06018695],
        [-0.05720121, -0.04758712, -0.06605697],
        [-0.04060118, -0.02268876, -0.03123908],
        [-0.05290632, -0.05539774, -0.06639562],
        [-0.04763426, -0.04088936, -0.0429609 ],
        [-0.05755835, -0.06501836, -0.04997568],
        [-0.05682862, -0.08448008, -0.08736774],
        [-0.06769446, -0.04170579, -0.06186615],
        [-0.05542174, -0.07509581, -0.06060786],
        [-0.04871209, -0.05045147, -0.05053867],
        [-0.12712141, -0.09735306, -0.10617105]]))
</code></pre><h4 id="&#x68AF;&#x5EA6;&#x7684;&#x5411;&#x91CF;&#x5316;&#x8BA1;&#x7B97;"><a href="#&#x68AF;&#x5EA6;&#x7684;&#x5411;&#x91CF;&#x5316;&#x8BA1;&#x7B97;" class="headerlink" title="&#x68AF;&#x5EA6;&#x7684;&#x5411;&#x91CF;&#x5316;&#x8BA1;&#x7B97;"></a>&#x68AF;&#x5EA6;&#x7684;&#x5411;&#x91CF;&#x5316;&#x8BA1;&#x7B97;</h4><p>&#x5B9E;&#x9645;&#x4E0A;&#xFF0C;&#x5982;&#x679C;&#x5BF9;&#x4E8E;&#x77E9;&#x9635;&#x6C42;&#x5BFC;&#x7684;&#x94FE;&#x5F0F;&#x6CD5;&#x5219;&#x8F83;&#x4E3A;&#x719F;&#x6089;&#x7684;&#x8BDD;&#xFF0C;&#x53EF;&#x4EE5;&#x66F4;&#x76F4;&#x63A5;&#x5730;&#x5F97;&#x5230;<code>dW</code>&#x7684;&#x5411;&#x91CF;&#x5316;&#x8BA1;&#x7B97;&#x516C;&#x5F0F;&#xFF1A;<br><!-- $$ --><br>\begin{equation}<br>    \nabla_{W} L<br>    :=\frac{dL}{dW}=<br>    \frac{dS}{dW} \frac{\partial L}{\partial S} + \frac{\partial L}{\partial W}<br>\end{equation}<br><!-- $$ --><br>&#x5176;&#x4E2D;<br><!-- $$ --><br>\begin{equation}<br>    \frac{\partial L}{\partial S} = \sum_{i=1}^N mask_i \quad,\quad \frac{\partial L}{\partial W} = 2\lambda W<br>\end{equation}<br><!-- $$ --><br>&#x800C;&#x6839;&#x636E;$S=X\cdot W$&#xFF0C;<br><!-- $$ --><br>\begin{equation}<br>    \frac{dS}{dW} \equiv X^T \quad,\quad \frac{dS}{dW} \frac{\partial L}{\partial S} = X^T \cdot \frac{dL}{dS}<br>\end{equation}<br><!-- $$ --></p>
<div class="note info no-icon"><p>&#x6CE8;&#xFF1A;&#x4E0A;&#x8FF0;&#x7ED3;&#x679C;&#x7684;&#x6B63;&#x786E;&#x6027;&#x53EF;&#x4EE5;&#x901A;&#x8FC7;&#x77E9;&#x9635;&#x7684;&#x7EF4;&#x5EA6;&#x8FDB;&#x884C;&#x95F4;&#x63A5;&#x9A8C;&#x8BC1;&#xFF1A;$X^T$&#x7684;&#x7EF4;&#x5EA6;&#x4E3A;$D\times N$&#xFF0C;$\frac{\partial L}{\partial S}$&#x4E0E;$S, mask_i$&#x7684;&#x7EF4;&#x5EA6;&#x76F8;&#x540C;&#xFF0C;&#x5747;&#x4E3A;$N\times C$&#xFF0C;&#x4E8E;&#x662F;$X^T \cdot \frac{\partial L}{\partial S}$&#x7684;&#x7EF4;&#x5EA6;&#x4E3A;$D\times C$&#xFF0C;&#x6070;&#x4E0E;$W$&#x7684;&#x7EF4;&#x5EA6;&#x76F8;&#x540C;&#xFF0C;&#x4E0E;&#x9884;&#x671F;&#x5B8C;&#x5168;&#x4E00;&#x81F4;&#x3002;&#x5229;&#x7528;&#x7EF4;&#x5EA6;&#x8FDB;&#x884C;&#x68AF;&#x5EA6;&#x7684;&#x6B63;&#x786E;&#x6027;&#x68C0;&#x9A8C;&#x5728;&#x8BA1;&#x7B97;&#x795E;&#x7ECF;&#x7F51;&#x7EDC;&#x7684;&#x53CD;&#x5411;&#x4F20;&#x64AD;&#x516C;&#x5F0F;&#x65F6;&#x5C06;&#x975E;&#x5E38;&#x9AD8;&#x6548;&#x3002;</p>
<p>&#x53E6;&#x5916;&#xFF0C;$\frac{dS}{dW}$&#x8FD9;&#x79CD;&#x5199;&#x6CD5;&#x662F;&#x4E0D;&#x4E25;&#x8C28;&#x7684;&#xFF0C;&#x8FD9;&#x91CC;&#x53EA;&#x662F;&#x4E3A;&#x4E86;&#x65B9;&#x4FBF;&#x4E66;&#x5199;&#xFF0C;&#x5C06;&#x8FDE;&#x63A5;$\frac{\partial L}{\partial S}$&#x548C;$\nabla_{W} L$&#x7B2C;&#x4E00;&#x9879;&#x7684;&#x67D0;&#x4E2A;&#x4E2D;&#x95F4;&#x53D8;&#x91CF;&#x6807;&#x8BB0;&#x4E3A;$\frac{dS}{dW}$&#x3002;</p>
</div>
<h3 id="&#x8BAD;&#x7EC3;"><a href="#&#x8BAD;&#x7EC3;" class="headerlink" title="&#x8BAD;&#x7EC3;"></a>&#x8BAD;&#x7EC3;</h3><p>&#x6700;&#x540E;&#x5199;&#x51FA;&#x4ECE;&#x968F;&#x673A;&#x751F;&#x6210;&#x56FE;&#x7247;&#x548C;&#x6807;&#x7B7E;&#xFF0C;&#x76F4;&#x5230;&#x8BAD;&#x7EC3;&#x5E76;&#x4F5C;&#x56FE;&#x7684;&#x4EE3;&#x7801;&#xFF1A;</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line">N = <span class="number">20</span>  <span class="comment"># &#x56FE;&#x7247;&#x6570;&#x91CF;</span></span><br><span class="line">C = <span class="number">4</span>   <span class="comment"># &#x5206;&#x7C7B;&#x6570;&#x91CF;</span></span><br><span class="line">D = <span class="number">3</span>   <span class="comment"># &#x56FE;&#x7247;&#x5C3A;&#x5BF8;</span></span><br><span class="line"><span class="comment"># &#x968F;&#x673A;&#x521D;&#x59CB;&#x5316;</span></span><br><span class="line">imgs = np.random.rand(N, D, D, <span class="number">3</span>)</span><br><span class="line">labels = np.random.randint(<span class="number">0</span>, C, size=N)</span><br><span class="line"></span><br><span class="line">imgs = imgs.reshape(N, -<span class="number">1</span>)</span><br><span class="line">imgs = np.hstack([imgs, np.ones((imgs.shape[<span class="number">0</span>], <span class="number">1</span>))])</span><br><span class="line">W = np.random.randn(imgs.shape[<span class="number">1</span>], C)</span><br><span class="line"></span><br><span class="line">loss_list, correct_rate_list = [], []</span><br><span class="line">T = np.arange(<span class="number">200</span>)  <span class="comment"># &#x8BAD;&#x7EC3;&#x6B21;&#x6570;</span></span><br><span class="line">alpha = <span class="number">0.2</span>         <span class="comment"># &#x5B66;&#x4E60;&#x7387;&#xFF08;&#x6B65;&#x957F;&#xFF09;</span></span><br><span class="line">delta = <span class="number">1</span>           <span class="comment"># &#x5B89;&#x5168;&#x8DDD;&#x79BB;</span></span><br><span class="line">reg = <span class="number">5e-5</span>          <span class="comment"># &#x6B63;&#x5219;&#x60E9;&#x7F5A;&#x5F3A;&#x5EA6;</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> t <span class="keyword">in</span> T:</span><br><span class="line">    loss, dW = svm_loss_grad(W, imgs, labels, delta=delta, reg=reg)</span><br><span class="line">    W -= alpha * dW</span><br><span class="line"></span><br><span class="line">    scores = np.dot(imgs, W)</span><br><span class="line">    labels_pred = np.argmax(scores, axis=<span class="number">1</span>)</span><br><span class="line">    correct_rate = np.<span class="built_in">sum</span>(labels_pred == labels) / <span class="built_in">len</span>(labels)</span><br><span class="line"></span><br><span class="line">    loss_list.append(loss)</span><br><span class="line">    correct_rate_list.append(correct_rate)</span><br><span class="line"></span><br><span class="line">fig = plt.figure()</span><br><span class="line">plt.subplot(<span class="number">1</span>,<span class="number">2</span>,<span class="number">1</span>)</span><br><span class="line">plt.plot(T, loss_list)</span><br><span class="line">plt.title(<span class="string">&quot;loss&quot;</span>)</span><br><span class="line"></span><br><span class="line">plt.subplot(<span class="number">1</span>,<span class="number">2</span>,<span class="number">2</span>)</span><br><span class="line">plt.plot(T, correct_rate_list)</span><br><span class="line">plt.title(<span class="string">&quot;correct_rate&quot;</span>)</span><br><span class="line"></span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<p><img src="/2021/10/14/SVM/loss_and_train.png" alt="png"></p>
<h2 id="&#x53C2;&#x8003;"><a href="#&#x53C2;&#x8003;" class="headerlink" title="&#x53C2;&#x8003;"></a>&#x53C2;&#x8003;</h2><ul>
<li>CS231n - Linear Classification<br><a href="https://cs231n.github.io/linear-classify/">https://cs231n.github.io/linear-classify/</a></li>
<li>Vectorized Implementation of SVM Loss and Gradient Update<br><a href="https://mlxai.github.io/2017/01/06/vectorized-implementation-of-svm-loss-and-gradient-update.html">https://mlxai.github.io/2017/01/06/vectorized-implementation-of-svm-loss-and-gradient-update.html</a></li>
</ul>
]]></content>
      <tags>
        <tag>ML</tag>
        <tag>CV</tag>
      </tags>
  </entry>
</search>
